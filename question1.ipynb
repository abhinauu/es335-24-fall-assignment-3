{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import streamlit as st\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(r\"C:\\Users\\abhin\\OneDrive\\Documents\\Cpp\\es335-24-fall-assignment-3\\shakespeare_input.txt\", \"r\").read()\n",
    "text = re.sub(r'[^a-zA-Z\\s.,!?:\\']+', '', text)\n",
    "text = re.sub(r'([.,!?:])', r' \\1 ', text)\n",
    "words = text.lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  28241\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(words)) + ['<UNK>']\n",
    "vocab_size = len(vocab)\n",
    "print(\"Length of vocabulary: \", vocab_size)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocab)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "padidx = word2idx['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextWordModel(nn.Module):\n",
    "    def __init__(self, emb_dim, context_len, actvn='ReLU'):\n",
    "        super(NextWordModel, self).__init__()\n",
    "        self.context_len = context_len\n",
    "        actdict = {'ReLU': nn.ReLU(), 'Tanh': nn.Tanh()}\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lin1 = nn.Linear(context_len * emb_dim, 256)\n",
    "        self.act1 = actdict[actvn]\n",
    "        self.lin2 = nn.Linear(256, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.lin1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "def get_ctxt(context_len):\n",
    "    X, y = [], []\n",
    "    context = [padidx]*context_len\n",
    "    for w in words:\n",
    "        idx = word2idx.get(w, word2idx['<UNK>'])\n",
    "        X.append(context)\n",
    "        y.append(idx)\n",
    "        context = context[1:] + [idx]\n",
    "    return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "def train(model, epochs, context_len):  \n",
    "    X, y = get_ctxt(context_len)\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    dataloder = torch.utils.data.DataLoader(dataset, pin_memory=True, num_workers=2, batch_size=128)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001)    \n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for x_batch, y_batch in dataloder:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1} Loss:\", sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "e32_c5_r = NextWordModel(32, 5, 'ReLU')\n",
    "e64_c5_r = NextWordModel(64, 5, 'ReLU')\n",
    "e32_c5_t = NextWordModel(32, 5, 'Tanh')\n",
    "e64_c5_t = NextWordModel(64, 5, 'Tanh')\n",
    "e32_c10_r = NextWordModel(32, 10, 'ReLU')\n",
    "e64_c10_r = NextWordModel(64, 10, 'ReLU')\n",
    "e32_c10_t = NextWordModel(32, 10, 'Tanh')\n",
    "e64_c10_t = NextWordModel(64, 10, 'Tanh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me32_c5_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(e32_c5_t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me32_c5_t.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epochs, context_len)\u001b[0m\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y_batch)\n\u001b[0;32m     42\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(e32_c5_t, 200,5)\n",
    "torch.save(e32_c5_t, \"e32_c5_t.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, num_words, temperature):\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?:\\']+', '', text)\n",
    "    text = re.sub(r'([.,!?:])', r' \\1 ', text)\n",
    "    text = text.lower().split()\n",
    "    context_len = model.context_len\n",
    "    generated_words = []\n",
    "    if len(text) < context_len:\n",
    "        context = [padidx]*(context_len - len(text)) + [word2idx.get(w, word2idx['<UNK>']) for w in text]\n",
    "    else:\n",
    "        context = [word2idx.get(w, word2idx['<UNK>']) for w in text[-context_len:]] \n",
    "    for _ in num_words:\n",
    "        x = torch.tensor(context).unsqueeze(0)\n",
    "        logits = model(x)\n",
    "        idx = torch.distributions.Categorical(logits=model(x)).sample().item()\n",
    "        next_word = idx2word[idx]\n",
    "        generated_words.append(next_word)\n",
    "        context = context[1:] + [idx]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(model):\n",
    "   embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "   tsne = TSNE(n_components=2, random_state=42)\n",
    "   embeddings_2d = tsne.fit_transform(embeddings)\n",
    "   selected_words = [\n",
    "       'love', 'hate',\n",
    "       'life', 'death',\n",
    "       'good', 'evil',\n",
    "       'light', 'dark',\n",
    "       'sweet', 'bitter',\n",
    "       'joy', 'sorrow',\n",
    "       'peace', 'war',\n",
    "       'heaven', 'hell',\n",
    "       'truth', 'lie',\n",
    "       'fair', 'foul',\n",
    "       'friend', 'foe',\n",
    "       'honor', 'shame',\n",
    "       'king', 'queen',\n",
    "       'lord', 'lady',\n",
    "       'day', 'night',\n",
    "       'young', 'old',\n",
    "       'laugh', 'weep'\n",
    "   ]\n",
    "   plt.figure(figsize=(15, 10))\n",
    "   plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "              c='lightgray', alpha=0.1, s=5)\n",
    "   for word in selected_words:\n",
    "       if word in word2idx:\n",
    "           idx = word2idx[word]\n",
    "           plt.scatter(embeddings_2d[idx, 0], embeddings_2d[idx, 1], \n",
    "                     c='blue', s=100)\n",
    "           plt.annotate(word, \n",
    "                      (embeddings_2d[idx, 0], embeddings_2d[idx, 1]),\n",
    "                      fontsize=10,\n",
    "                      alpha=0.8)\n",
    "   \n",
    "   plt.title(f'Word Embeddings Visualization (dim={model.embedding.embedding_dim})')\n",
    "   plt.axis('off')\n",
    "   plt.tight_layout()\n",
    "   plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
